# architecture
arch: null
enc_arch: VisionTransformer
dec_arch: UNETR_decoder

# wandb
proj_name: UNETR3D
run_name: "seg_test"
wandb_id:
disable_wandb: True
use_tensorboard: true

# dataset
dataset: btcv
json_list: "seg_demo.json"
data_path: "examples/data"
cache_rate: 0.0

# output
output_root: "runs"
save_seg: true

# data preprocessing
z_score: true
space_x: 1
space_y: 1
space_z: 1
a_min: 0.0
a_max: 4000000.0
b_min: 0.0
b_max: 1.0
roi_x: 48
roi_y: 48
roi_z: 48
RandFlipd_prob: 0.2
RandRotate90d_prob: 0.2
RandScaleIntensityd_prob: 0.1
RandShiftIntensityd_prob: 0.1
infer_overlap: 0.5
spatial_dim: 3
num_samples: 4

# trainer
trainer_name: SegTrainer
batch_size: 6
val_batch_size: 1 # per gpu
start_epoch: 0
warmup_epochs: 50
epochs: 1000
workers: 4
pretrain: null
resume: "weights/seg.pth.tar"
test: true

# drop
drop_path: 0.1
# tricks
mixup: 0.
cutmix: 0.
label_smoothing: 0.

# model
# patchembed: 'PatchEmbed3D'
# pos_embed_type: 'sincos'
# mask_ratio: 0.75
patch_size: 8
in_chans: 1
feature_size: 16
encoder_embed_dim: 768
encoder_depth: 12
encoder_num_heads: 12
# decoder_embed_dim: 384
# decoder_depth: 8
# decoder_num_heads: 12

# loss
smooth_nr: 0.0
smooth_dr: 1e-6

# optimizer
type: adamw
lr: 4.3e-3
beta1: 0.9
beta2: 0.95 #0.999
weight_decay: 0.05 #1e-5
layer_decay: 0.75

# logging
# vis_freq: 100
vis_batch_size: 4
save_freq: 10
eval_freq: 10
print_freq: 1



# randomness
seed: 6666

# debugging
debug: false


# ======================================
# ↓↓↓↓↓↓↓↓↓↓↓ GPU and DDP ↓↓↓↓↓↓↓↓↓↓↓↓↓↓
gpu: 0 # if `multiprocessing_distributed` is true, this arg will be ignored
multiprocessing_distributed: false # true: Default use all gpus. Use `export CUDA_VISIBLE_DEVICES=0,2` to choose gpus.
ngpus_per_node: null # if not set, will use all gpus
rank: 0 # the ID of this node
world_size: 1 # the number of nodes
distributed: null # will be replaced: args.distributed = args.world_size > 1 or args.multiprocessing_distributed
dist_backend: nccl
dist_url: 'tcp://localhost:10011'
# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑
# =====================================
